\documentclass[french]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[a4paper]{geometry}
\usepackage{babel}

\usepackage{amsmath}
\usepackage{bbm}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{float}
\usepackage{geometry}
\geometry{hmargin=3.5cm,vmargin=3cm}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{eurosym}

\title{Homework 1}
\author{ \textbf{Graphical models} \\Vincent Henric \& Antoine Lepeltier }
\date{}
\begin{document}
	
	\maketitle
	
	\section{Learning in discrete graphical models}
\noindent
For an i.i.d. sample $(X,Z)_i$ with size $N$ the likelihood is given by 
\begin{align*}
L(\pi,\theta | X,Z ) &= \prod_{i=1}^N L(\pi,\theta | X_i,Z_i ) \\
&=  \prod_{i=1}^N p(X_i=x_i, Z_i=z_i ) \\
&=  \prod_{i=1}^N p(X_i=x_i | Z_i=z_i ) p(Z_i=z_i)\\
&=  \prod_{i=1}^N \theta_{x_iz_i} \pi_{z_i}
\end{align*}
So the log-likelihood is
\begin{align*}
\ell (\pi,\theta | X,Z ) &= \sum_{i=1}^N \log(\theta_{x_iz_i} \pi_{z_i}) \\
&= \sum_{i=1}^N \log(\theta_{x_iz_i}) + \log( \pi_{z_i}) \\
&= \sum_{i=1}^N  \sum_{m=1}^M  \log( \pi_{m}) 1_{z_i=m} + \sum_{i=1}^N \sum_{m=1}^M  \sum_{k=1}^K\log(\theta_{mk}) 1_{z_i=m} 1_{x_i=k}\\
&= \sum_{m=1}^M  \log( \pi_{m})  \sum_{i=1}^N 1_{z_i=m} + \sum_{m=1}^M  \sum_{k=1}^K\log(\theta_{mk}) \sum_{i=1}^N 1_{z_i=m} 1_{x_i=k}\\
&= \sum_{m=1}^M  \log( \pi_{m})  N_m + \sum_{m=1}^M  \sum_{k=1}^K\log(\theta_{mk}) N_{mk}\\
\end{align*}
with $\sum_{i=1}^N 1_{z_i=m} = N_m$ and $\sum_{i=1}^N 1_{z_i=m} 1_{x_i=k}=N_{mk}$
We want to maximize the log-likelihood function, we can maximize each term separetly. Let's consider first $\sum_{m=1}^M  \log( \pi_{m})  N_m$.\\
Our problem become
\begin{align*}
\min  -&\sum_{m=1}^M  \log( \pi_{m})  N_m \text{ (sum of convex function so it's convex)}\\
\text{s. t. }  &\sum_{m=1}^M \pi_m = 1 
\end{align*}
It become a convex optimization problem with strong duality property from \textit{Slaterâ€™s constraint qualification}. Let's compute the lagrangian
\begin{align*}
Lg(\pi,\nu) &= - \sum_{m=1}^M N_m log{\pi_m} + \nu(\sum_{m=1}^M \pi_m - 1)
\end{align*}
It's a convex function so we can calculate the derivative for find the minimum in $\pi$
\begin{align*}
\frac{\partial Lg(\pi,\nu)}{\partial \pi_m} &= -\frac{N_m}{\pi_m} + \nu\\
\text{so } \frac{\partial Lg(\pi,\nu)}{\partial \pi_m}  &= 0 \implies \pi_m = \frac{N_m}{\nu}
\end{align*}
From the constrait condition $\sum_{m=1}^M \pi_m =1$ we got
\begin{align*}
\sum_{m=1}^M \frac{N_m}{\nu} = 1 \implies  \nu = \sum_{m=1}^M N_m = N
\end{align*}
Hence $\pi_m = \frac{N_m}{\nu} = \frac{N_m}{N}$
\bigskip \\
For the second term $ \sum_{m=1}^M  \sum_{k=1}^K\log(\theta_{mk}) N_{mk}$, we can look to the following optimization problem 
\begin{align*}
\min - &\sum_{m=1}^M  \sum_{k=1}^K\log(\theta_{mk}) N_{mk} \\
\text{s. t. } &\sum_{k=1}^K \theta_{mk} = 1 ~~~ \forall m \in \{1,2,...,M\}
\end{align*}
The lagragian is
\begin{align*}
Lg(\pi,\nu) &= - \sum_{m=1}^M  \sum_{k=1}^K\log(\theta_{mk}) N_{mk} + \sum_{m=1}^M \nu_m (\sum_{k=1}^K \theta_{mk} - 1)
\end{align*}
$Lg(\pi,\nu)$ is a convex function so for minimize it we can look into his derivative and we got the minimum at $\theta_{mk} = \frac{N_{mk}}{\nu_m}$.
Hence, by using the constraint condition $\forall m \sum_{k=1}^K \theta_{mk} =1$, we got
\begin{align*}
&\forall m \sum_{k=1}^K = \frac{N_{mk}}{\nu_m} \\
&  \forall m ~\nu_m = N_m
\end{align*}
Hence $\theta_{mk} = \frac{N_{mk}}{N_m}$
\bigskip\\
The maximum likelihood is given by 
\begin{equation*}
\boxed{  \pi_m = \frac{N_m}{\nu} = \frac{N_m}{N} \text{ and } \theta_{mk} = \frac{N_{mk}}{N_m} }
\end{equation*}



\end{document}
